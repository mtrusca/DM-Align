# DMSEdit
DMSEdit for semantic image editing


We thank the reviewers for their instructive comments. Their questions and remarks are written in italic font.

Weaknesses: 
1. *The datasets in DiffEdit paper are available.* The authors of DiffEdit use 3 datasets. 1. ***The 1st dataset is a subset of ImageNet that is not available.*** The authors only provide some guidance on how to obtain the subset. We did not follow their setup as the text instructions of ImageNet contain only one object (noun). One of aims of our paper is to prove that our model works well for long and elaborate text captions. 2. ***The 2nd dataset is composed of images generated by Imagen using template prompts. This dataset is not available.*** 3. The 3rd dataset relies on the BISON dataset (defined based on COCO). The dataset is used for a binary selection of an image based on a caption. As BISON is not defined for image editing, the dataset should be preprocessed to be suitable for image editing. ***The preprocessed BISON dataset used by DiffEdit is not available.***
2.  *The experimental setting is overly simplified.* ***As our method is defined based on word alignments, the source and the target instructions should have a certain level of similarity. Because it is difficult to select a fixed similarity threshold (measured with ROUGE), we present results for different thresholds.***
3.  *The restriction to nouns (with adjectives) limits the scope of editing possibilities.* ***We clearly stated this limitation in our paper and we would like to address the editing based on position changes in the feature work.***
4.	*Assuming that complicated and elaborated text captions refer to "longer length" and "more chunks," why not demonstrate the results across various lengths/numbers of chunks within the same dataset?* ***We selected two subsets of BISON_07 edited images considering the number of chunks equal to 1 and 4.*** Based on the tables below, the difference between the image-based metrics (especially LPIPS and PWMSE) is higher when the number of chunks is 4 than when the number is 1. ***These results prove that the proposed model is especially effective for elaborate text instructions (with a larger number of chunks).*** These results are consistent with the results presented in the paper. These results will be added in the Appendix.

|no chunks=1     | FID ↓     |LPIPS ↓	 |PWMSE ↓	  |CLIPScore↑|      
|FlexIT          |70.23±0.43 |0.34±0.00 |39.87±0.06 |0.87±0.00 |
|DiffEdit        |83.53±0.24 |0.34±0.00 |40.23±3.98 |0.78±0.00 |
|ControlNet      |79.78±0.19 |0.35±0.00 |42.61±0.98 |0.78±0.00 |
|Prompt-to-Prompt|           |          |           |0.77±0.00 |
|DM-Align        |59.97±0.78 |0.33±0.00 |35.74±0.67 |0.78±0.00 |

# Requirements

Install the required libraries/packages.

```python
pip install -r requirements.txt
```
```python
git clone https://github.com/facebookresearch/detectron2
```
```python
python -m spacy download en_core_web_md
```

# Instalation

Download the best ViCHA checkpoint from https://github.com/mshukor/ViCHA and move it in the folder vicha.

Download the Stanford Parser from https://nlp.stanford.edu/software/lex-parser.shtml in the folder parser.

_Optional_: Generate new word alignments using https://github.com/chaojiang06/neural-Jacana.

# Data

Download the images, captions and alignments in the folder data.
- BISON 0.7
  - Images: [Link](https://drive.google.com/drive/folders/18RKSWSs42q3xq2Y6JHl4_AZKlnvxpJc4?usp=share_link)
  - Captions: [Link](https://drive.google.com/file/d/1mPOeQajLRzHRLS6DYiNUXLJNZjCTE6t4/view?usp=share_link)
  - Alignments: [Link](https://drive.google.com/file/d/1XVJGXNfjmAVapjPTSfr38O6OGJ_qpWOQ/view?usp=share_link)
- Dream
  - Images: [Link](https://drive.google.com/drive/folders/1RazlDU43B26N8HFZxBVYfqmerznZectH?usp=share_link)
  - Captions: [Link](https://drive.google.com/file/d/1fCEWqlJVgxw1ysPLEyMJ1yUCNdWUbOGo/view?usp=share_link)
  - Alignments: [Link](https://drive.google.com/file/d/1doxV4_65gE4RG8nrNZA9fUvTFEZCOJ2-/view?usp=share_link)

# Edit images with DMSEdit

BISON 07 dataset

```python
python /content/drive/MyDrive/dmsedit/run_dmsedit.py \
--path_input_data './data/bison_07.json'  \
--path_source_images './data/bison_07/' \
--path_alignments './data/alignments_bison_07.json'  \
--vicha_model_path './vicha/checkpoint_best.pth'  \
--bert_config_path './configs/config_bert.json'  \
--path_to_jar './parser/stanford-parser.jar' \
--path_to_models_jar './parser/stanford-parser-4.2.0-models.jar' \
--path_target_images './output_bison_07/'  \
--token ' '  \
--cache_dir "./cache_dir/"
```

Dream dataset

```python
python /content/drive/MyDrive/dmsedit/run_dmsedit.py \
--path_input_data './data/dream.json'  \
--path_source_images './data/dream/' \
--path_alignments './data/alignments_dream.json'  \
--vicha_model_path './vicha/checkpoint_best.pth'  \
--bert_config_path './configs/config_bert.json'  \
--path_to_jar './parser/stanford-parser.jar' \
--path_to_models_jar './parser/stanford-parser-4.2.0-models.jar' \
--path_target_images './output_dream/'  \
--token ' '  \
--cache_dir "./cache_dir/"
```
Note: token represents the Hugging Face token required for Stable Diffusion.

# Quick example

Add a new image in the folder examples and edit it using ```DMSEdit_example.ipynb ```.

# Evaluation

Install the required libraries/packages.

```python
pip install lpips
```
```python
pip install pytorch_fid
```
```python
pip install git+https://github.com/openai/CLIP.git
```
Run the script ```evaluation_dmsedit.py```

